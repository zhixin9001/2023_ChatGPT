æœ‰äº†ChatGPTçš„å¸®åŠ©ï¼Œå°±å¯ä»¥ä¿¡å¿ƒåè¶³åœ°æ¶‰ç•¥ä¸€äº›ä¹‹å‰æ²¡ç»éªŒåœ°é¢†åŸŸäº†ã€‚
è¿™æ¬¡çš„ä¿é™©æ¬ºè¯ˆè¯†åˆ«ä»»åŠ¡ï¼Œæ•°æ®æ¥æºäºKaggleçš„ç«èµ›é¢˜ç›®ï¼šhttps://www.kaggle.com/competitions/fraud-detection-in-insurance-claims
å»ºæ¨¡è¯­è¨€ä½¿ç”¨Python3ï¼Œå¹¶ä½¿ç”¨Jupyteræ¥ç»„ç»‡ç»™GPTçš„Promoteå’Œä»£ç ï¼Œå®Œæˆåå¯ä»¥ç›´æ¥å¯¼å‡ºä¸ºmarkdownã€‚
æœ€ç»ˆé¢„æµ‹çš„æˆç»©ä¸º0.88885, æœ€é«˜åˆ†0.91244ï¼Œå·²ç»å¾ˆæ»¡æ„äº†ğŸ˜
![result](./Capture.PNG)
æ¥ä¸‹æ¥æ˜¯æ•°æ®åˆ†æå’Œå»ºæ¨¡çš„è¿‡ç¨‹ï¼Œä¸ºäº†ç¼©çŸ­ç¯‡å¹…ï¼ŒGPTçš„å›ç­”éƒ¨åˆ†åªä¿ç•™äº†ä»£ç ã€‚

é—®ï¼šä½ ç°åœ¨æ˜¯ä¸€ä¸ªæ•°æ®å»ºæ¨¡äººå‘˜ï¼Œè¦å¯¹ä¿é™©åæ¬ºè¯ˆè¿›è¡Œé¢„æµ‹ã€‚
æ•°æ®åŒ…æ‹¬ï¼š
1ï¼‰è®­ç»ƒé›†ï¼štrain.csv ï¼ˆ40ä¸ªå­—æ®µï¼Œæœ‰fraud_reportedå­—æ®µï¼‰
2ï¼‰æµ‹è¯•é›†ï¼štest.csv ï¼ˆ39ä¸ªå­—æ®µï¼Œæ²¡æœ‰fraud_reportedå­—
æ®µï¼‰

PS: ChatGPTçš„ç¬¬ä¸€æ¬¡å›ç­”ä¸­åŒ…å«äº†é€šç”¨çš„å»ºæ¨¡æ­¥éª¤,è¿™ä¹Ÿæ˜¯æœ€æ ¸å¿ƒçš„éƒ¨åˆ†.
```python
ä½œä¸ºæ•°æ®å»ºæ¨¡äººå‘˜ï¼Œå¯¹ä¿é™©åæ¬ºè¯ˆè¿›è¡Œé¢„æµ‹æ˜¯ä¸€ä¸ªæœ‰è¶£çš„ä»»åŠ¡ã€‚æˆ‘å¯ä»¥å¸®åŠ©ä½ åˆ¶å®šä¸€ä¸ªåŸºæœ¬çš„å»ºæ¨¡æµç¨‹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªé€šç”¨çš„æ­¥éª¤ï¼š

1. å¯¼å…¥å¿…è¦çš„åº“å’ŒåŠ è½½æ•°æ®é›†ï¼š
   - ä½¿ç”¨é€‚åˆæ•°æ®åˆ†æå’Œå»ºæ¨¡çš„åº“ï¼Œå¦‚Pandasã€NumPyå’ŒScikit-learnã€‚
   - ä½¿ç”¨Pandasåº“çš„`read_csv`å‡½æ•°åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®ã€‚

2. æ•°æ®æ¢ç´¢å’Œé¢„å¤„ç†ï¼š
   - ä½¿ç”¨Pandasåº“çš„å„ç§å‡½æ•°å’Œæ–¹æ³•æ¥äº†è§£æ•°æ®é›†çš„ç»“æ„ã€ç‰¹å¾å’Œåˆ†å¸ƒã€‚
   - æ£€æŸ¥ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼ï¼Œå¹¶æ ¹æ®éœ€è¦è¿›è¡Œå¤„ç†ã€‚
   - è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼ŒåŒ…æ‹¬ç‰¹å¾é€‰æ‹©ã€ç‰¹å¾å˜æ¢ã€åˆ›å»ºæ–°ç‰¹å¾ç­‰ã€‚

3. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼š
   - å°†è®­ç»ƒé›†æ•°æ®åˆ’åˆ†ä¸ºç”¨äºè®­ç»ƒæ¨¡å‹çš„å­é›†å’Œç”¨äºéªŒè¯æ¨¡å‹æ€§èƒ½çš„å­é›†ã€‚
   - è¿™å¯ä»¥é€šè¿‡Scikit-learnåº“ä¸­çš„`train_test_split`å‡½æ•°å®ç°ã€‚

4. é€‰æ‹©æ¨¡å‹å’Œè®­ç»ƒï¼š
   - æ ¹æ®é—®é¢˜çš„ç‰¹ç‚¹é€‰æ‹©é€‚å½“çš„åˆ†ç±»æ¨¡å‹ï¼Œå¦‚é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ç­‰ã€‚
   - ä½¿ç”¨è®­ç»ƒé›†ä¸Šçš„æ•°æ®å¯¹æ‰€é€‰æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚

5. æ¨¡å‹è¯„ä¼°ï¼š
   - ä½¿ç”¨éªŒè¯é›†ä¸Šçš„æ•°æ®è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚
   - å¸¸è§çš„è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ã€‚

6. æ¨¡å‹ä¼˜åŒ–å’Œè°ƒå‚ï¼š
   - æ ¹æ®æ¨¡å‹æ€§èƒ½å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–å’Œè°ƒå‚ï¼Œä»¥æ”¹å–„é¢„æµ‹æ•ˆæœã€‚
   - å¯ä»¥ä½¿ç”¨äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢ç­‰æŠ€æœ¯æ¥æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆã€‚

7. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼š
   - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆé¢„æµ‹ç»“æœã€‚

è¯·æ³¨æ„ï¼Œä¸Šè¿°æ­¥éª¤æ˜¯ä¸€ä¸ªé€šç”¨çš„æµç¨‹ç¤ºä¾‹ï¼Œå…·ä½“å®ç°å¯èƒ½å› æ•°æ®é›†çš„ç‰¹ç‚¹å’Œå»ºæ¨¡æ–¹æ³•çš„é€‰æ‹©è€Œæœ‰æ‰€ä¸åŒã€‚å¦å¤–ï¼Œæ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹é€‰æ‹©ç­‰ç¯èŠ‚ä¹Ÿå¯èƒ½éœ€è¦æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œè¿›ä¸€æ­¥çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚
```

é¦–å…ˆéœ€è¦ä½ å¯¹æ•°æ®è¿›è¡ŒåŠ è½½ï¼Œåˆ†åˆ«å¾—åˆ° trainå’Œtestã€‚ç»™æˆ‘å¯¹åº”çš„Pythonä»£ç 


```python
import pandas as pd

# åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

# æŸ¥çœ‹è®­ç»ƒé›†æ•°æ®çš„å‰å‡ è¡Œ
print("è®­ç»ƒé›†æ•°æ®ï¼š")
print(train_data.head())

# æŸ¥çœ‹æµ‹è¯•é›†æ•°æ®çš„å‰å‡ è¡Œ
print("æµ‹è¯•é›†æ•°æ®ï¼š")
print(test_data.head())

# æ£€æŸ¥æ•°æ®é›†çš„ç»´åº¦
print("è®­ç»ƒé›†æ•°æ®ç»´åº¦ï¼š", train_data.shape)
print("æµ‹è¯•é›†æ•°æ®ç»´åº¦ï¼š", test_data.shape)
```

å¯¹train_dataå’Œtest_dataè¿›è¡Œåˆå¹¶ï¼Œç”Ÿæˆdata


```python
# åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†
data = pd.concat([train_data, test_data], ignore_index=True)

# æŸ¥çœ‹åˆå¹¶åçš„æ•°æ®æ¡†
print("åˆå¹¶åçš„æ•°æ®ï¼š")
print(data.head())

# æ£€æŸ¥åˆå¹¶åçš„æ•°æ®æ¡†ç»´åº¦
print("åˆå¹¶åçš„æ•°æ®ç»´åº¦ï¼š", data.shape)
```

å¯¹dataè¿›è¡Œæ•°æ®æ¢ç´¢ï¼ŒåŒ…æ‹¬ï¼šç¼ºå¤±å€¼ï¼Œå­—æ®µçš„å”¯ä¸€å€¼ä¸ªæ•°


```python
# æ£€æŸ¥æ¯ä¸ªå­—æ®µçš„ç¼ºå¤±å€¼æ•°é‡
missing_values = data.isnull().sum()
print("å­—æ®µçš„ç¼ºå¤±å€¼æ•°é‡ï¼š")
print(missing_values)

```


```python
# è®¡ç®—æ¯ä¸ªå­—æ®µçš„å”¯ä¸€å€¼ä¸ªæ•°
unique_value_counts = data.nunique()
print("å­—æ®µçš„å”¯ä¸€å€¼ä¸ªæ•°ï¼š")
print(unique_value_counts)
```

åœ¨dataä¸­ï¼Œå»æ‰æ²¡æœ‰çš„å­—æ®µ '_c39'


```python
# å»é™¤å­—æ®µ '_c39'
data = data.drop('_c39', axis=1)

# æ£€æŸ¥æ›´æ–°åçš„æ•°æ®æ¡†
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data.head())
```

ç­›é€‰dataä¸­çš„å­—ç¬¦ä¸²å­—æ®µï¼Œèµ‹å€¼ç»™cat_columns


```python
# ç­›é€‰å‡ºå­—ç¬¦ä¸²å­—æ®µ
cat_columns = data.select_dtypes(include=['object'])

# æŸ¥çœ‹ç­›é€‰å‡ºçš„å­—ç¬¦ä¸²å­—æ®µ
print("å­—ç¬¦ä¸²å­—æ®µï¼š")
print(cat_columns.head())

```

å¯¹dataä¸­çš„è¿™äº›å­—ç¬¦ä¸²å­—æ®µï¼Œè®¡ç®—å”¯ä¸€å€¼ä¸ªæ•°ï¼Œå¹¶æŒ‰ç…§å”¯ä¸€å€¼ä¸ªæ•°ä»å¤§åˆ°å°æ’åº


```python
# è®¡ç®—å­—ç¬¦ä¸²å­—æ®µçš„å”¯ä¸€å€¼ä¸ªæ•°
unique_counts = data.select_dtypes(include=['object']).nunique()

# æŒ‰ç…§å”¯ä¸€å€¼ä¸ªæ•°ä»å¤§åˆ°å°æ’åº
unique_counts_sorted = unique_counts.sort_values(ascending=False)

# æŸ¥çœ‹æ’åºåçš„å”¯ä¸€å€¼ä¸ªæ•°
print("æŒ‰ç…§å”¯ä¸€å€¼ä¸ªæ•°æ’åºåï¼š")
print(unique_counts_sorted)
```

dataä¸­çš„property_damageå­—æ®µæœ‰3ç§å€¼ï¼Œåˆ†åˆ«ä¸ºï¼šNO, YES å’Œ ?ï¼Œåˆ†åˆ«å°†ä»–ä»¬æ˜ å°„æˆ0, 1, 2


```python
# å°†ä¸‰ç§å€¼æ˜ å°„ä¸º 0, 1, 2
data['property_damage'] = data['property_damage'].replace({"NO": 0, "YES": 1, "?": 2})

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®æ¡†
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data['property_damage'].head())

```

å¯¹dataä¸­çš„police_report_availableå­—æ®µæœ‰3ç§å€¼ï¼Œåˆ†åˆ«ä¸ºï¼šNO, YES å’Œ ?ï¼Œåˆ†åˆ«å°†ä»–ä»¬æ˜ å°„æˆ0, 1, 2


```python
# å°†ä¸‰ç§å€¼æ˜ å°„ä¸º 0, 1, 2
data['police_report_available'] = data['police_report_available'].replace({"NO": 0, "YES": 1, "?": 2})

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®æ¡†
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data['police_report_available'].head())

```

dataä¸­æœ‰ä¸¤ä¸ªå­—æ®µæ˜¯æ—¥æœŸç±»å‹ï¼Œåˆ†åˆ«ä¸ºpolicy_bind_date, incident_dateï¼Œä¸ºç¡®ä¿ä»–ä»¬æ˜¯æ—¥æœŸç±»å‹å­—æ®µï¼Œéœ€è¦å°†ä»–ä»¬æ ¼å¼è¿›è¡Œè½¬æ¢


```python
# å°†å­—æ®µè½¬æ¢ä¸ºæ—¥æœŸç±»å‹
data['policy_bind_date'] = pd.to_datetime(data['policy_bind_date'])
data['incident_date'] = pd.to_datetime(data['incident_date'])

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®ç±»å‹
print("æ›´æ–°åçš„æ•°æ®ç±»å‹ï¼š")
print(data[['policy_bind_date', 'incident_date']])
```


```python
# æ‰¾åˆ° policy_bind_date çš„æœ€å°æ—¥æœŸ
min_policy_bind_date = data['policy_bind_date'].min()
# è®¡ç®—æ¯ä¸ªè®°å½•çš„ policy_bind_date_diff
data['policy_bind_date_diff'] = (data['policy_bind_date'] - min_policy_bind_date).dt.days

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data[['policy_bind_date', 'policy_bind_date_diff']].head())
```

åŒç†ï¼Œåœ¨dataä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„å­—æ®µincident_date_diffï¼Œå®ƒç­‰äº (incident_date - policy_bind_dateä¸­æ—¥æœŸæœ€å°å€¼)çš„å¤©æ•°


```python

# è®¡ç®—æ¯ä¸ªè®°å½•çš„ incident_date_diff
data['incident_date_diff'] = (data['incident_date'] - min_policy_bind_date).dt.days

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data[['incident_date', 'incident_date_diff']].head())
```

åœ¨dataä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„å­—æ®µincident_date_policy_bind_date_diffï¼Œå®ƒç­‰äºincident_date_diff - policy_bind_date_diff


```python
# åˆ›å»ºæ–°çš„å­—æ®µ incident_date_policy_bind_date_diff
data['incident_date_policy_bind_date_diff'] = data['incident_date_diff'] - data['policy_bind_date_diff']

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data[['incident_date_diff', 'policy_bind_date_diff', 'incident_date_policy_bind_date_diff']].head())

```

åœ¨dataä¸­å»æ‰åŸå§‹çš„æ—¥æœŸå­—æ®µ policy_bind_dateå’Œincident_date


```python
# å»æ‰åŸå§‹çš„æ—¥æœŸå­—æ®µ
data = data.drop(['policy_bind_date', 'incident_date'], axis=1)

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®æ¡†
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data.head())
```

åœ¨dataä¸­æœ‰å­—ç¬¦ä¸²ç±»å‹çš„å­—æ®µï¼Œéœ€è¦å¯¹ä»–ä»¬è¿›è¡Œæ ‡ç­¾ç¼–ç 


```python
from sklearn.preprocessing import LabelEncoder

# é‡æ–°ç­›é€‰å‡ºå­—ç¬¦ä¸²å­—æ®µ
data1=data;
cat_columns = data1.select_dtypes(include=['object'])

# åˆ›å»º LabelEncoder å¯¹è±¡
label_encoder = LabelEncoder()


# éå†å­—ç¬¦ä¸²å­—æ®µï¼Œè¿›è¡Œæ ‡ç­¾ç¼–ç 
for column in cat_columns:
    data1[column] = label_encoder.fit_transform(data1[column])

# æŸ¥çœ‹æ›´æ–°åçš„æ•°æ®
print("æ›´æ–°åçš„æ•°æ®ï¼š")
print(data1.head())

```

ä¹‹å‰dataæ˜¯ç”±train_dataå’Œtest_dataåˆå¹¶çš„ï¼Œç°åœ¨å†å°†dataæ‹†åˆ†å›train_dataå’Œtest_data


```python
# æ ¹æ®ä¹‹å‰çš„ç´¢å¼•æˆ–è¡Œæ•°æ‹†åˆ†å› train_data å’Œ test_data
train_data = data1[:len(train_data)]
test_data = data1[len(train_data):]

# æŸ¥çœ‹æ‹†åˆ†åçš„æ•°æ®
print("æ‹†åˆ†åçš„ train_dataï¼š")
print(train_data.shape)

print("æ‹†åˆ†åçš„ test_dataï¼š")
print(test_data.shape)

```

ç°åœ¨æƒ³è¦ç”¨LightGBMè¿›è¡Œå»ºæ¨¡ï¼Œå¯¹train_dataè¿›è¡Œè®­ç»ƒï¼Œå¯¹test_dataè¿›è¡Œé¢„æµ‹ï¼Œå…¶ä¸­ fraud_reportedæ˜¯ç›®æ ‡å­—æ®µï¼Œé¢„æµ‹ç»“æœå¯ä»¥æ˜¯æ¦‚ç‡å€¼


```python
import lightgbm as lgb
from sklearn.metrics import roc_auc_score

# å®šä¹‰ç‰¹å¾å’Œç›®æ ‡å­—æ®µ
features = train_data.drop('fraud_reported', axis=1)
target = train_data['fraud_reported']

# åˆ›å»º LightGBM æ•°æ®é›†
train_dataset = lgb.Dataset(features, label=target)

# å®šä¹‰ LightGBM å‚æ•°
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': 0
}

# åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ LightGBM æ¨¡å‹
model = lgb.train(params, train_dataset, num_boost_round=100)

# å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
test_features = test_data.drop('fraud_reported', axis=1)
test_pred = model.predict(test_features)

# è¾“å‡ºé¢„æµ‹ç»“æœï¼ˆæ¦‚ç‡å€¼ï¼‰
print("é¢„æµ‹ç»“æœï¼š")
print(test_pred)

```

è¯»å–sampleSubmission.csvï¼Œå°†fraud_reportedå­—æ®µè®¾ç½®ä¸ºåˆšæ‰å¾—åˆ°çš„ test_predï¼Œå¹¶ä¿å­˜ä¸ºresult_baseline.csv


```python
# è¯»å– sampleSubmission.csv æ–‡ä»¶
submission = pd.read_csv("sampleSubmission.csv")

# å°† fraud_reported å­—æ®µè®¾ç½®ä¸ºé¢„æµ‹ç»“æœ test_pred
submission['fraud_reported'] = test_pred

# ä¿å­˜ç»“æœä¸º result_baseline.csv æ–‡ä»¶
submission.to_csv("result_baseline.csv", index=False)

# æ‰“å°ä¿å­˜æˆåŠŸçš„æç¤ºä¿¡æ¯
print("ç»“æœå·²ä¿å­˜ä¸º result_baseline.csv æ–‡ä»¶")
```
